{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4622366a",
   "metadata": {},
   "source": [
    "\n",
    "# IEOR E4004 — Project I (Model 2 Only): Realistic Expansion & Location (Gurobi)\n",
    "\n",
    "This notebook loads the provided CSVs, prepares populations and spacing conflicts, and **builds & solves the Model 2** (realistic expansion with piecewise expansion cost and distance constraints).  \n",
    "It will export a CSV summary named `model_2_summary.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b971cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 1) Imports & Global Config\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "DATA_DIR = Path(\"C:/Users/evere/Desktop/研究生课程/4004_Opt/ChildCareDeserts_Data\")  # adjust if needed\n",
    "\n",
    "# New facility sizes and costs\n",
    "NEW_FACILITY_SIZES = {\n",
    "    \"small\":  {\"total\": 100, \"max_0_5\": 50,  \"cost\": 65000},\n",
    "    \"medium\": {\"total\": 200, \"max_0_5\": 100, \"cost\": 95000},\n",
    "    \"large\":  {\"total\": 400, \"max_0_5\": 200, \"cost\": 115000},\n",
    "}\n",
    "\n",
    "# Distance threshold in miles\n",
    "MIN_DISTANCE_MILES = 0.06\n",
    "\n",
    "# Employment/Income thresholds for high-demand ZIPs\n",
    "EMPLOYMENT_RATE_CUTOFF = 0.60\n",
    "AVG_INCOME_CUTOFF = 60000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "900228c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 2) Utility Functions\n",
    "\n",
    "def haversine_miles(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Haversine distance in miles between two lat/lon points (degrees).\"\"\"\n",
    "    R = 3958.8  # Earth radius in miles\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2.0)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2.0)**2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "def safe_series(df, col):\n",
    "    \"\"\"Return a Series if column exists, else zeros.\"\"\"\n",
    "    return df[col] if col in df.columns else 0\n",
    "\n",
    "def compute_populations(pop_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute population aggregates by zipcode for:\n",
    "    - pop_0_5 ≈ pop['0-4'] + (1/5)*pop['5-9']\n",
    "    - pop_5_12 ≈ pop['5-9'] + (3/5)*pop['10-14']\n",
    "    - pop_0_12 = pop_0_5 + pop_5_12\n",
    "    \"\"\"\n",
    "    df = pop_df.copy()\n",
    "    # Ensure numeric for all non-ZIP columns\n",
    "    for c in df.columns:\n",
    "        if c not in [\"zipcode\", \"ZIP code\", \"zip_code\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "    # Normalize ZIP column name\n",
    "    if \"zipcode\" not in df.columns:\n",
    "        if \"ZIP code\" in df.columns:\n",
    "            df.rename(columns={\"ZIP code\": \"zipcode\"}, inplace=True)\n",
    "        elif \"zip_code\" in df.columns:\n",
    "            df.rename(columns={\"zip_code\": \"zipcode\"}, inplace=True)\n",
    "\n",
    "    pop_0_4 = safe_series(df, \"0-4\")\n",
    "    pop_5_9 = safe_series(df, \"5-9\")\n",
    "    pop_10_14 = safe_series(df, \"10-14\")\n",
    "    pop_0_5 = pop_0_4 + (1.0/5.0)*pop_5_9\n",
    "    pop_5_12 = pop_5_9 + (3.0/5.0)*pop_10_14\n",
    "    pop_0_12 = pop_0_5 + pop_5_12\n",
    "    return df[[\"zipcode\"]].assign(pop_0_5=pop_0_5, pop_5_12=pop_5_12, pop_0_12=pop_0_12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583c7b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\mnt\\\\data\\\\child_care_regulated.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Existing facilities\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m ccare \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_care_regulated.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Normalize zip column name\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip_code\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ccare\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\mnt\\\\data\\\\child_care_regulated.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 3) Load & Prepare Data\n",
    "\n",
    "DATA_DIR = Path(\"/mnt/data\")\n",
    "\n",
    "# Existing facilities\n",
    "ccare = pd.read_csv(DATA_DIR / \"child_care_regulated.csv\")\n",
    "\n",
    "# Normalize zip column name\n",
    "if \"zip_code\" in ccare.columns:\n",
    "    ccare[\"zipcode\"] = ccare[\"zip_code\"]\n",
    "elif \"ZIP code\" in ccare.columns:\n",
    "    ccare[\"zipcode\"] = ccare[\"ZIP code\"]\n",
    "\n",
    "# Ensure numeric capacities\n",
    "for col in [\"infant_capacity\", \"toddler_capacity\", \"preschool_capacity\",\n",
    "            \"school_age_capacity\", \"children_capacity\", \"total_capacity\"]:\n",
    "    if col in ccare.columns:\n",
    "        ccare[col] = pd.to_numeric(ccare[col], errors=\"coerce\").fillna(0.0)\n",
    "    else:\n",
    "        ccare[col] = 0.0\n",
    "ccare[\"latitude\"] = pd.to_numeric(ccare[\"latitude\"], errors=\"coerce\")\n",
    "ccare[\"longitude\"] = pd.to_numeric(ccare[\"longitude\"], errors=\"coerce\")\n",
    "\n",
    "# Potential new locations\n",
    "locs = pd.read_csv(DATA_DIR / \"potential_locations.csv\")\n",
    "if \"zipcode\" not in locs.columns and \"ZIP code\" in locs.columns:\n",
    "    locs.rename(columns={\"ZIP code\": \"zipcode\"}, inplace=True)\n",
    "\n",
    "# Population\n",
    "pop_raw = pd.read_csv(DATA_DIR / \"population.csv\")\n",
    "pop = compute_populations(pop_raw)\n",
    "\n",
    "# Income\n",
    "inc = pd.read_csv(DATA_DIR / \"avg_individual_income.csv\")\n",
    "if \"ZIP code\" in inc.columns:\n",
    "    inc.rename(columns={\"ZIP code\": \"zipcode\", \"average income\": \"avg_income\"}, inplace=True)\n",
    "if \"average income\" in inc.columns and \"avg_income\" not in inc.columns:\n",
    "    inc.rename(columns={\"average income\": \"avg_income\"}, inplace=True)\n",
    "\n",
    "# Employment rate\n",
    "emp = pd.read_csv(DATA_DIR / \"employment_rate.csv\")\n",
    "if \"employment rate\" in emp.columns and \"employment_rate\" not in emp.columns:\n",
    "    emp.rename(columns={\"employment rate\": \"employment_rate\"}, inplace=True)\n",
    "\n",
    "# ZIP-level merged table\n",
    "zip_df = pop.merge(inc[[\"zipcode\", \"avg_income\"]], on=\"zipcode\", how=\"left\")\n",
    "zip_df[\"avg_income\"] = pd.to_numeric(zip_df[\"avg_income\"], errors=\"coerce\")\n",
    "zip_df[\"employment_rate\"] = pd.to_numeric(zip_df[\"employment_rate\"], errors=\"coerce\")\n",
    "zip_df.fillna({\"avg_income\": np.inf, \"employment_rate\": 0.0}, inplace=True)\n",
    "\n",
    "# Aggregate existing capacity per facility and by zip\n",
    "ccare[\"cap_0_5\"] = ccare[[\"infant_capacity\", \"toddler_capacity\", \"preschool_capacity\"]].sum(axis=1)\n",
    "ccare[\"cap_5_12\"] = ccare[\"school_age_capacity\"]\n",
    "ccare[\"cap_total\"] = ccare[\"total_capacity\"].replace({np.nan: 0.0})\n",
    "mask_total_zero = (ccare[\"cap_total\"].isna()) | (ccare[\"cap_total\"] <= 0)\n",
    "ccare.loc[mask_total_zero, \"cap_total\"] = (ccare[\"cap_0_5\"].fillna(0) + ccare[\"cap_5_12\"].fillna(0))\n",
    "\n",
    "# Ensure an ID per facility (string)\n",
    "if \"facility_id\" not in ccare.columns:\n",
    "    ccare[\"facility_id\"] = np.arange(len(ccare))\n",
    "ccare[\"facility_id\"] = ccare[\"facility_id\"].astype(str)\n",
    "\n",
    "print(f\"Facilities: {len(ccare)} | Potential locations: {len(locs)} | ZIPs: {len(zip_df)}\")\n",
    "ccare.head(2), locs.head(2), zip_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d563ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 4) Build Spacing Conflicts within ZIPs (0.06 miles)\n",
    "\n",
    "def build_conflicts(ccare: pd.DataFrame, locs: pd.DataFrame, min_miles=MIN_DISTANCE_MILES):\n",
    "    locs_clean = locs.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "    ccare_clean = ccare.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "    locs_clean[\"loc_id\"] = locs_clean.index.astype(int)\n",
    "\n",
    "    new_new_conflicts = {}\n",
    "    new_exist_conflicts = {}\n",
    "\n",
    "    for z, group in locs_clean.groupby(\"zipcode\"):\n",
    "        gps = group[[\"loc_id\", \"latitude\", \"longitude\"]].values.tolist()\n",
    "        pairs = []\n",
    "        for i in range(len(gps)):\n",
    "            for j in range(i+1, len(gps)):\n",
    "                id_i, la_i, lo_i = gps[i]\n",
    "                id_j, la_j, lo_j = gps[j]\n",
    "                d = haversine_miles(la_i, lo_i, la_j, lo_j)\n",
    "                if d < min_miles:\n",
    "                    pairs.append((int(id_i), int(id_j)))\n",
    "        new_new_conflicts[z] = pairs\n",
    "\n",
    "        exist_pairs = []\n",
    "        ex = ccare_clean[ccare_clean[\"zipcode\"] == z]\n",
    "        for _, r in group.iterrows():\n",
    "            for _, e in ex.iterrows():\n",
    "                d = haversine_miles(r[\"latitude\"], r[\"longitude\"], e[\"latitude\"], e[\"longitude\"])\n",
    "                if d < min_miles:\n",
    "                    exist_pairs.append((int(r[\"loc_id\"]), str(e[\"facility_id\"])))\n",
    "        new_exist_conflicts[z] = exist_pairs\n",
    "\n",
    "    return locs_clean, new_new_conflicts, new_exist_conflicts\n",
    "\n",
    "locs_clean, new_new_conflicts, new_exist_conflicts = build_conflicts(ccare, locs, MIN_DISTANCE_MILES)\n",
    "print(\"Conflicts built.\")\n",
    "print(\"Example new-new conflicts count (first 3 zips):\",\n",
    "      [(z, len(pairs)) for z, pairs in list(new_new_conflicts.items())[:3]])\n",
    "print(\"Example new-existing conflicts count (first 3 zips):\",\n",
    "      [(z, len(pairs)) for z, pairs in list(new_exist_conflicts.items())[:3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 5) Build & Solve Model 2 (Realistic Expansion & Location)\n",
    "\n",
    "def solve_model_2(ccare, locs, zip_df, new_new_conflicts, new_exist_conflicts):\n",
    "    m = Model(\"Model_2_Realistic\")\n",
    "\n",
    "    F = list(ccare[\"facility_id\"])\n",
    "    facility_nf = dict(zip(F, ccare[\"cap_total\"]))\n",
    "    facility_zip = dict(zip(F, ccare[\"zipcode\"]))\n",
    "\n",
    "    L = list(locs.index)\n",
    "    loc_zip = dict(zip(L, locs[\"zipcode\"]))\n",
    "    Z = list(zip_df[\"zipcode\"])\n",
    "\n",
    "    # Decision variables\n",
    "    x1 = {f: m.addVar(lb=0.0, ub=0.10*facility_nf[f], vtype=GRB.CONTINUOUS, name=f\"x1[{f}]\") for f in F}\n",
    "    x2 = {f: m.addVar(lb=0.0, ub=0.05*facility_nf[f], vtype=GRB.CONTINUOUS, name=f\"x2[{f}]\") for f in F}\n",
    "    x3 = {f: m.addVar(lb=0.0, ub=0.05*facility_nf[f], vtype=GRB.CONTINUOUS, name=f\"x3[{f}]\") for f in F}\n",
    "\n",
    "    y = {(l, s): m.addVar(vtype=GRB.BINARY, name=f\"y_build[{l},{s}]\") for l in L for s in NEW_FACILITY_SIZES}\n",
    "\n",
    "    s0_5 = {z: m.addVar(lb=0.0, vtype=GRB.CONTINUOUS, name=f\"s0_5[{z}]\") for z in Z}\n",
    "    s5_12 = {z: m.addVar(lb=0.0, vtype=GRB.CONTINUOUS, name=f\"s5_12[{z}]\") for z in Z}\n",
    "    a_new_0_5 = {z: m.addVar(lb=0.0, vtype=GRB.CONTINUOUS, name=f\"a_new0_5[{z}]\") for z in Z}\n",
    "    b_new_5_12 = {z: m.addVar(lb=0.0, vtype=GRB.CONTINUOUS, name=f\"b_new5_12[{z}]\") for z in Z}\n",
    "\n",
    "    m.update()\n",
    "\n",
    "    # Helpers\n",
    "    cap0_5_zip = ccare.groupby(\"zipcode\")[\"cap_0_5\"].sum().to_dict()\n",
    "    cap5_12_zip = ccare.groupby(\"zipcode\")[\"cap_5_12\"].sum().to_dict()\n",
    "    cap_total_zip = ccare.groupby(\"zipcode\")[\"cap_total\"].sum().to_dict()\n",
    "\n",
    "    pop_0_5 = dict(zip(zip_df[\"zipcode\"], zip_df[\"pop_0_5\"]))\n",
    "    pop_5_12 = dict(zip(zip_df[\"zipcode\"], zip_df[\"pop_5_12\"]))\n",
    "    pop_0_12 = dict(zip(zip_df[\"zipcode\"], zip_df[\"pop_0_12\"]))\n",
    "    avg_income = dict(zip(zip_df[\"zipcode\"], zip_df[\"avg_income\"]))\n",
    "    emp_rate = dict(zip(zip_df[\"zipcode\"], zip_df[\"employment_rate\"]))\n",
    "\n",
    "    # Constraints\n",
    "    for f in F:\n",
    "        nf = facility_nf[f]\n",
    "        m.addConstr(x1[f] + x2[f] + x3[f] <= 0.20 * nf, name=f\"x_total_cap[{f}]\")\n",
    "\n",
    "    for z in Z:\n",
    "        exp_in_z = quicksum(x1[f] + x2[f] + x3[f] for f in F if facility_zip[f] == z)\n",
    "        new_tot_in_z = quicksum(NEW_FACILITY_SIZES[s][\"total\"] * y[l, s] for l in L if loc_zip[l] == z for s in NEW_FACILITY_SIZES)\n",
    "\n",
    "        m.addConstr(s0_5[z] + s5_12[z] == cap_total_zip.get(z, 0.0) + exp_in_z + new_tot_in_z, name=f\"alloc_balance[{z}]\")\n",
    "        m.addConstr(s0_5[z] <= cap0_5_zip.get(z, 0.0) + exp_in_z + a_new_0_5[z], name=f\"max_0_5_bound[{z}]\")\n",
    "        m.addConstr(a_new_0_5[z] <= quicksum(NEW_FACILITY_SIZES[s][\"max_0_5\"] * y[l, s] for l in L if loc_zip[l] == z for s in NEW_FACILITY_SIZES), name=f\"new_0_5_cap[{z}]\")\n",
    "        m.addConstr(a_new_0_5[z] + b_new_5_12[z] <= new_tot_in_z, name=f\"new_split_tot[{z}]\")\n",
    "\n",
    "        high_demand = (emp_rate.get(z, 0.0) >= EMPLOYMENT_RATE_CUTOFF) or (avg_income.get(z, np.inf) <= AVG_INCOME_CUTOFF)\n",
    "        threshold = 0.5 if high_demand else (1.0/3.0)\n",
    "        m.addConstr(s0_5[z] + s5_12[z] >= threshold * pop_0_12.get(z, 0.0), name=f\"desert_off[{z}]\")\n",
    "        m.addConstr(s0_5[z] >= (2.0/3.0) * pop_0_5.get(z, 0.0), name=f\"policy_0_5[{z}]\")\n",
    "\n",
    "    # Distance conflicts\n",
    "    for z, pairs in new_new_conflicts.items():\n",
    "        for (i, j) in pairs:\n",
    "            m.addConstr(quicksum(y[i, s] for s in NEW_FACILITY_SIZES) + quicksum(y[j, s] for s in NEW_FACILITY_SIZES) <= 1, name=f\"dist_new_new[{z},{i},{j}]\")\n",
    "    for l in L:\n",
    "        m.addConstr(quicksum(y[l, s] for s in NEW_FACILITY_SIZES) <= 1, name=f\"one_per_loc[{l}]\")\n",
    "    for z, pairs in new_exist_conflicts.items():\n",
    "        for (i, f) in pairs:\n",
    "            m.addConstr(quicksum(y[i, s] for s in NEW_FACILITY_SIZES) <= 0, name=f\"dist_new_exist_block[{z},{i},{f}]\")\n",
    "\n",
    "    # Objective\n",
    "    expand_cost = quicksum( ((20000.0 / facility_nf[f]) + 200.0) * x1[f] +\n",
    "                            ((20000.0 / facility_nf[f]) + 400.0) * x2[f] +\n",
    "                            ((20000.0 / facility_nf[f]) + 1000.0) * x3[f]\n",
    "                           for f in F if facility_nf[f] > 0 )\n",
    "    new_build_cost = quicksum( NEW_FACILITY_SIZES[s][\"cost\"] * y[l, s] for l in L for s in NEW_FACILITY_SIZES )\n",
    "    m.setObjective(expand_cost + new_build_cost, GRB.MINIMIZE)\n",
    "\n",
    "    m.Params.OutputFlag = 1\n",
    "    m.optimize()\n",
    "\n",
    "    status = m.Status\n",
    "    obj = m.ObjVal if status in (GRB.OPTIMAL, GRB.SUBOPTIMAL) else None\n",
    "\n",
    "    # Export summary\n",
    "    rows = []\n",
    "    for z in Z:\n",
    "        rows.append({\n",
    "            \"zipcode\": z,\n",
    "            \"s0_5\": s0_5[z].X if s0_5[z].X is not None else 0.0,\n",
    "            \"s5_12\": s5_12[z].X if s5_12[z].X is not None else 0.0,\n",
    "        })\n",
    "    summary = pd.DataFrame(rows)\n",
    "    summary[\"model_obj_total_cost\"] = obj\n",
    "    summary.to_csv(\"model_2_summary.csv\", index=False)\n",
    "\n",
    "    return {\"status\": status, \"objective\": obj, \"summary\": summary, \"model\": m}\n",
    "\n",
    "res2 = solve_model_2(ccare, locs_clean, zip_df, new_new_conflicts, new_exist_conflicts)\n",
    "print(f\"Model 2 status: {res2['status']} | Min total funding: {res2['objective']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb342371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 6) Inspect Results\n",
    "\n",
    "import pandas as pd\n",
    "m2 = pd.read_csv(\"model_2_summary.csv\")\n",
    "m2.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
